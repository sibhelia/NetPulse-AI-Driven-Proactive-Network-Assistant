import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
import joblib
import os
import sys

# ==========================================
# âš™ï¸ AYARLAR VE DOSYA YOLLARI
# ==========================================
# Kodun Ã§alÄ±ÅŸtÄ±ÄŸÄ± yere gÃ¶re yollarÄ± dinamik bul
BASE_DIR = os.path.dirname(os.path.abspath(__file__)) # Åu anki dosyanÄ±n yeri
PROJECT_ROOT = os.path.dirname(BASE_DIR)              # Proje ana klasÃ¶rÃ¼

# Girdi: Ham Veri
RAW_DATA_PATH = os.path.join(PROJECT_ROOT, 'data', 'netpulse_telemetry_final.csv')

# Ã‡Ä±ktÄ±: Ä°ÅŸlenmiÅŸ Veriler ve Modeller
PROCESSED_DATA_DIR = os.path.join(PROJECT_ROOT, 'data', 'processed')
ARTIFACTS_DIR = os.path.join(PROJECT_ROOT, 'models', 'saved_objects')

# KlasÃ¶rleri oluÅŸtur
os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)
os.makedirs(ARTIFACTS_DIR, exist_ok=True)

print("ğŸš€ Veri Ã–niÅŸleme (Preprocessing) BaÅŸlÄ±yor...")
print(f"ğŸ“‚ Ham Veri: {RAW_DATA_PATH}")

# ==========================================
# 1. ADIM: VERÄ°YÄ° YÃœKLE
# ==========================================
try:
    df = pd.read_csv(RAW_DATA_PATH)
    print(f"âœ… Veri YÃ¼klendi. Boyut: {df.shape}")
except FileNotFoundError:
    print("âŒ HATA: Ham veri dosyasÄ± bulunamadÄ±! LÃ¼tfen Ã¶nce 'generate_data.py' Ã§alÄ±ÅŸtÄ±rÄ±n.")
    sys.exit()

# ==========================================
# 2. ADIM: EÄÄ°TÄ°M VE TEST AYRIMI (SPLITTING)
# ==========================================
# Ã–nemli Kural: Scaler ve Encoder'Ä± eÄŸitmeden Ã¶nce veriyi ayÄ±rmalÄ±yÄ±z.
# BÃ¶ylece Test setindeki bilgiler eÄŸitim sÃ¼recine sÄ±zmaz (Data Leakage Ã–nleme).

X = df.drop(columns=['root_cause', 'label', 'timestamp', 'subscriber_id', 'region_id']) # Girdiler
y = df['root_cause'] # Hedef (Ã‡Ä±ktÄ±)

# Stratify=y diyerek her arÄ±za tipinden (y) eÄŸitim ve test setine eÅŸit oranda daÄŸÄ±tÄ±yoruz.
print("âœ‚ï¸  Veri EÄŸitim (%80) ve Test (%20) olarak ayrÄ±lÄ±yor...")
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ==========================================
# 3. ADIM: ENCODING (KATEGORÄ°K -> SAYISAL)
# ==========================================
print("ğŸ”¢ Kategorik veriler kodlanÄ±yor...")

# infra_type sÃ¼tununu (Fiber/VDSL) sayÄ±ya Ã§evir
le_infra = LabelEncoder()

# Sadece train setindeki deÄŸerleri Ã¶ÄŸren (fit), sonra hem train hem test'i dÃ¶nÃ¼ÅŸtÃ¼r (transform)
X_train['infra_type_encoded'] = le_infra.fit_transform(X_train['infra_type'])
X_test['infra_type_encoded'] = le_infra.transform(X_test['infra_type'])

# ArtÄ±k string olan 'infra_type' sÃ¼tununa ihtiyacÄ±mÄ±z yok
X_train = X_train.drop(columns=['infra_type'])
X_test = X_test.drop(columns=['infra_type'])

# Encoder'Ä± kaydet (API'de kullanacaÄŸÄ±z)
joblib.dump(le_infra, os.path.join(ARTIFACTS_DIR, 'infra_encoder.pkl'))

# ==========================================
# 4. ADIM: SCALING (STANDARTLAÅTIRMA)
# ==========================================
print("âš–ï¸  SayÄ±sal veriler Ã¶lÃ§ekleniyor (StandardScaler)...")

# Ã–lÃ§eklenecek sÃ¼tunlar (TÃ¼m sayÄ±sal sÃ¼tunlar)
numeric_cols = [
    'distance_to_cabinet_m', 'download_usage_mbps', 'upload_usage_mbps',
    'signal_strength_rssi', 'latency_ms', 'jitter_ms', 'packet_loss_ratio',
    'snr_margin_db', 'modem_cpu_usage', 'modem_ram_usage'
]

scaler = StandardScaler()

# Scaler'Ä± SADECE X_TRAIN Ã¼zerinde eÄŸit (fit)
scaler.fit(X_train[numeric_cols])

# Ã–ÄŸrenilen ortalama ve sapmayÄ± kullanarak hem Train hem Test'i dÃ¶nÃ¼ÅŸtÃ¼r
X_train[numeric_cols] = scaler.transform(X_train[numeric_cols])
X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])

# Scaler'Ä± kaydet (Ã‡ok Ã¶nemli!)
joblib.dump(scaler, os.path.join(ARTIFACTS_DIR, 'scaler.pkl'))

# ==========================================
# 5. ADIM: Ä°ÅLENMÄ°Å VERÄ°YÄ° BÄ°RLEÅTÄ°R VE KAYDET
# ==========================================
print("ğŸ’¾ Ä°ÅŸlenmiÅŸ veriler kaydediliyor...")

# X ve y'yi tekrar birleÅŸtirip CSV olarak kaydedelim ki train.py kolayca okusun
train_df = pd.concat([X_train, y_train], axis=1)
test_df = pd.concat([X_test, y_test], axis=1)

train_path = os.path.join(PROCESSED_DATA_DIR, 'train_data.csv')
test_path = os.path.join(PROCESSED_DATA_DIR, 'test_data.csv')

train_df.to_csv(train_path, index=False)
test_df.to_csv(test_path, index=False)

print(f"âœ… Ä°ÅLEM TAMAMLANDI!")
print(f"   EÄŸitim Seti: {train_path} ({train_df.shape})")
print(f"   Test Seti:   {test_path} ({test_df.shape})")
print(f"   Objeler:     {ARTIFACTS_DIR} klasÃ¶rÃ¼ne kaydedildi.")