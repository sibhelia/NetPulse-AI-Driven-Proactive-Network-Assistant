import pandas as pd
import numpy as np
import joblib
import os
import sys
import time
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# --- 1. YOL AYARLARI (KRÄ°TÄ°K KISIM) ---
# Åu anki dosya: src/models/train_model.py
# 3 Ã¼st klasÃ¶re Ã§Ä±karsak proje ana dizinine (NetPulse) geliriz.
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Veri Yolu
DATA_PATH = os.path.join(BASE_DIR, 'data', 'processed', 'train_data.csv')

# Model KayÄ±t Yeri (ARTIK ANA DÄ°ZÄ°NDEKÄ° saved_models)
MODEL_DIR = os.path.join(BASE_DIR, 'saved_models')

# KlasÃ¶r yoksa oluÅŸtur
os.makedirs(MODEL_DIR, exist_ok=True)

print("ğŸš€ NetPulse AI (Random Forest) EÄŸitim ModÃ¼lÃ¼ BaÅŸlatÄ±lÄ±yor...")
print(f"ğŸ“‚ Hedef KlasÃ¶r: {MODEL_DIR}")

def train_rf_model():
    start_time = time.time()

    # --- 2. VERÄ° YÃœKLEME ---
    if not os.path.exists(DATA_PATH):
        # Yedek plan: Processed yoksa ham veriye bak
        ALT_DATA_PATH = os.path.join(BASE_DIR, 'data', 'netpulse_telemetry_final.csv')
        if os.path.exists(ALT_DATA_PATH):
             print(f"âš ï¸ Processed veri bulunamadÄ±, ham veri kullanÄ±lÄ±yor: {ALT_DATA_PATH}")
             df = pd.read_csv(ALT_DATA_PATH)
        else:
            print(f"âŒ HATA: Veri dosyasÄ± bulunamadÄ±! ({DATA_PATH})")
            sys.exit()
    else:
        print(f"ğŸ“Š Veri yÃ¼kleniyor: {DATA_PATH}")
        df = pd.read_csv(DATA_PATH)

    # --- 3. Ã–N Ä°ÅLEME ---
    # Gereksiz sÃ¼tunlarÄ± at (EÄŸer varsa)
    drop_cols = ['timestamp', 'device_id', 'modem_temperature', 'customer_id'] 
    df = df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore')

    # Hedef ve Ã–zellikleri AyÄ±r
    target_col = 'root_cause'
    if target_col not in df.columns:
        print(f"âŒ HATA: '{target_col}' sÃ¼tunu bulunamadÄ±.")
        return

    X = df.drop(columns=[target_col])
    y = df[target_col]

    # Hedef DeÄŸiÅŸkeni Kodla (Label Encoding: String -> SayÄ±)
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    
    # SÄ±nÄ±f isimlerini saklayalÄ±m
    class_names = list(le.classes_)
    print(f"ğŸ¯ Hedef SÄ±nÄ±flar: {class_names}")

    # EÄŸitim/Test AyrÄ±mÄ±
    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)

    # --- 4. PIPELINE KURULUMU (Profesyonel Standart) ---
    # SayÄ±sal ve Kategorik sÃ¼tunlarÄ± otomatik bul
    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns
    categorical_features = X.select_dtypes(include=['object']).columns

    # DÃ¶nÃ¼ÅŸtÃ¼rÃ¼cÃ¼ler
    numeric_transformer = StandardScaler()
    categorical_transformer = OneHotEncoder(handle_unknown='ignore')

    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_features),
            ('cat', categorical_transformer, categorical_features)
        ])

    # Pipeline: Ã–nce Ä°ÅŸle -> Sonra EÄŸit
    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('classifier', RandomForestClassifier(random_state=42))
    ])

    # --- 5. GRID SEARCH (HÄ°PERPARAMETRE OPTÄ°MÄ°ZASYONU) ---
    print("\nğŸ” Grid Search ile en iyi parametreler aranÄ±yor...")
    
    param_grid = {
        'classifier__n_estimators': [100, 200],      # AÄŸaÃ§ sayÄ±sÄ±
        'classifier__max_depth': [None, 10, 20],    # Derinlik
        'classifier__min_samples_split': [2, 5],
        'classifier__class_weight': ['balanced', None]
    }

    grid_search = GridSearchCV(
        pipeline, 
        param_grid, 
        cv=3, 
        n_jobs=-1, 
        scoring='f1_macro',
        verbose=1
    )
    
    grid_search.fit(X_train, y_train)

    # --- 6. SONUÃ‡LAR VE RAPOR ---
    elapsed_time = time.time() - start_time
    print(f"\nâœ… EÄŸitim TamamlandÄ±! SÃ¼re: {elapsed_time:.2f} sn")
    print(f"ğŸ† En Ä°yi Parametreler: {grid_search.best_params_}")
    print(f"ğŸŒŸ En Ä°yi CV Skoru: {grid_search.best_score_:.4f}")

    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(X_test)

    print("\n" + "="*60)
    print("SINIFLANDIRMA RAPORU")
    print("="*60)
    print(classification_report(y_test, y_pred, target_names=class_names))

    # --- 7. KAYDETME (CRITICAL FIX) ---
    # Modeli kaydet (Pipeline olduÄŸu iÃ§in scaler iÃ§inde!)
    model_path = os.path.join(MODEL_DIR, 'netpulse_classifier.pkl')
    joblib.dump(best_model, model_path)
    
    # Label Encoder'Ä± kaydet (Ã‡Ä±ktÄ±yÄ± 'Modem ArÄ±zasÄ±' diye okumak iÃ§in ÅŸart)
    encoder_path = os.path.join(MODEL_DIR, 'infra_encoder.pkl')
    joblib.dump(le, encoder_path)
    
    print(f"\nğŸ’¾ DOSYALAR KAYDEDÄ°LDÄ°:")
    print(f"   1. Model:   {model_path}")
    print(f"   2. Encoder: {encoder_path}")

if __name__ == "__main__":
    train_rf_model()