import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix, f1_score
import joblib
import os
import sys
import time

# AYARLAR VE YOL TANIMLARI
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(BASE_DIR)

# Ä°ÅŸlenmiÅŸ verileri buradan alacaÄŸÄ±z
TRAIN_DATA_PATH = os.path.join(PROJECT_ROOT, 'data', 'processed', 'train_data.csv')
TEST_DATA_PATH = os.path.join(PROJECT_ROOT, 'data', 'processed', 'test_data.csv')

# Modeli buraya kaydedeceÄŸiz
MODEL_DIR = os.path.join(PROJECT_ROOT, 'models', 'saved_objects')
MODEL_PATH = os.path.join(MODEL_DIR, 'netpulse_classifier.pkl')

print(" NetPulse GeliÅŸmiÅŸ AI EÄŸitim ModÃ¼lÃ¼ BaÅŸlatÄ±lÄ±yor...")
print(" Hedef: En iyi hiperparametreleri bulmak ve Cross-Validation uygulamak.")

# ==========================================
# 1. ADIM: Ä°ÅLENMÄ°Å VERÄ°YÄ° YÃœKLE
# ==========================================
if not os.path.exists(TRAIN_DATA_PATH):
    print(" HATA: Ä°ÅŸlenmiÅŸ veri bulunamadÄ±! LÃ¼tfen Ã¶nce 'src/preprocessing.py' Ã§alÄ±ÅŸtÄ±rÄ±n.")
    sys.exit()

print("\n Veriler yÃ¼kleniyor...")
train_df = pd.read_csv(TRAIN_DATA_PATH)
test_df = pd.read_csv(TEST_DATA_PATH)

# Ã–zellikler (X) ve Hedef (y) ayrÄ±mÄ±
target_col = 'root_cause'
X_train = train_df.drop(columns=[target_col])
y_train = train_df[target_col]

X_test = test_df.drop(columns=[target_col])
y_test = test_df[target_col]

print(f"EÄŸitim Seti: {X_train.shape}")
print(f"Test Seti:   {X_test.shape}")

# ==========================================
# 2. ADIM: HÄ°PERPARAMETRE IZGARASI (GRID)
# ==========================================
# Modeli rastgele eÄŸitmek yerine, en iyi ayarlarÄ± deneyerek bulacaÄŸÄ±z.
# JÃ¼ri Notu: Bu kÄ±sÄ±m "Model Tuning" yetkinliÄŸini gÃ¶sterir.

param_grid = {
    'n_estimators': [100, 200],       # KaÃ§ tane karar aÄŸacÄ± olsun?
    'max_depth': [None, 10, 20],      # AÄŸaÃ§lar ne kadar derinleÅŸsin? (Ezberlemeyi Ã¶nlemek iÃ§in)
    'min_samples_split': [2, 5],      # Bir dalÄ±n ikiye ayrÄ±lmasÄ± iÃ§in en az kaÃ§ veri lazÄ±m?
    'class_weight': ['balanced', None] # Dengesiz veriyi (az gÃ¶rÃ¼len arÄ±zalar) Ã¶nemse
}

# ==========================================
# 3. ADIM: CROSS-VALIDATION Ä°LE EÄÄ°TÄ°M
# ==========================================
print("\n  Grid Search & Cross Validation BaÅŸlÄ±yor...")
print("   (Bu iÅŸlem en iyi modeli bulmak iÃ§in veriyi defalarca eÄŸitir, biraz sÃ¼rebilir...)")

start_time = time.time()

# Temel Model
rf = RandomForestClassifier(random_state=42)

# Ã‡apraz DoÄŸrulama Stratejisi (StratifiedKFold)
# Veriyi 5 parÃ§aya bÃ¶lÃ¼yoruz. Her parÃ§ada arÄ±za oranlarÄ±nÄ±n eÅŸit olmasÄ±nÄ± saÄŸlÄ±yoruz.
cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Grid Search Nesnesi
grid_search = GridSearchCV(
    estimator=rf,
    param_grid=param_grid,
    cv=cv_strategy,
    scoring='f1_macro', # ArÄ±zalarÄ± yakalamak (Recall) ve doÄŸru bilmek (Precision) dengesi
    n_jobs=-1,          # TÃ¼m iÅŸlemci Ã§ekirdeklerini kullan
    verbose=1
)

# EÄŸitimi BaÅŸlat
grid_search.fit(X_train, y_train)

elapsed_time = time.time() - start_time
print(f"\nâœ¨ EÄŸitim TamamlandÄ±! SÃ¼re: {elapsed_time:.2f} saniye")
print(f"ğŸ† EN Ä°YÄ° PARAMETRELER: {grid_search.best_params_}")
print(f"ğŸ† EN Ä°YÄ° CV SKORU (F1-Macro): {grid_search.best_score_:.4f}")

# En iyi modeli seÃ§
best_model = grid_search.best_estimator_

# ==========================================
# 4. ADIM: TEST SETÄ° ÃœZERÄ°NDE FÄ°NAL SINAV
# ==========================================
print("\nğŸ” Final Testi (HiÃ§ GÃ¶rÃ¼lmemiÅŸ Veri Ä°le)...")
y_pred = best_model.predict(X_test)

# DetaylÄ± Rapor
print("\n" + "="*60)
print("ğŸ“Š SINIFLANDIRMA RAPORU (CLASSIFICATION REPORT)")
print("="*60)
print(classification_report(y_test, y_pred))

# Confusion Matrix (Metin bazlÄ± basit gÃ¶sterim)
print("\nğŸ§© KARMAÅIKLIK MATRÄ°SÄ° (CONFUSION MATRIX)")
cm = confusion_matrix(y_test, y_pred)
print(cm)

# ==========================================
# 5. ADIM: Ã–ZELLÄ°K Ã–NEMÄ° (FEATURE IMPORTANCE) - "JÃœRÄ° AVCISI"
# ==========================================
# Modelin hangi veriye bakarak karar verdiÄŸini aÃ§Ä±klar.
print("\n" + "="*60)
print("ğŸŒŸ Ã–ZELLÄ°K Ã–NEM DÃœZEYLERÄ° (EXPLAINABLE AI)")
print("="*60)

feature_importances = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': best_model.feature_importances_
}).sort_values(by='Importance', ascending=False)

print(feature_importances)

# En Ã¶nemli 3 nedeni yorumlayalÄ±m (Otomatik Yorumlama)
top_feature = feature_importances.iloc[0]['Feature']
print(f"\nğŸ’¡ SONUÃ‡: Yapay zeka kararlarÄ±nÄ± en Ã§ok '{top_feature}' verisine bakarak veriyor.")

# ==========================================
# 6. ADIM: KAYDETME
# ==========================================
print(f"\nğŸ’¾ En iyi model kaydediliyor: {MODEL_PATH}")
joblib.dump(best_model, MODEL_PATH)

print("âœ… BÃœTÃœN SÃœREÃ‡LER BAÅARIYLA TAMAMLANDI.")